<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>A-Closer-Look-at-FSIG</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">A Closer Look at Few-shot Image Generation</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<td align=center width=180px>
					<center>
						<span style="font-size:20px"><a href="https://yunqing-me.github.io/">Yunqing Zhao</a></span>
					</center>
				</td>
				<td align=center width=180px>
					<center>
						<span style="font-size:20px"><a href="https://henghuiding.github.io/">Henghui Ding</a></span>
					</center>
				</td>
				<td align=center width=180px>
					<center>
						<span style="font-size:20px"><a href="https://huanghoujing.github.io/">Houjing Huang</a></span>
					</center>
				</td>
				<td align=center width=250px>
					<center>
						<span style="font-size:20px"><a href="https://sites.google.com/site/mancheung0407/">Ngai-Man Cheung</a></span>
					</center>
				</td>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=600px>
						<center>
							<span style="font-size:20px">Singapore University of Technology and Design (SUTD) </a></span>
						</center>
					</td>
				</tr>
				<tr>
					<td align=center width=600px>
						<center>
							<span style="font-size:20px">ETH Z&uuml;rich &nbsp; &nbsp;&nbsp; ByteDance Ltd.</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=550px>
				<tr>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href='https://arxiv.org/abs/2205.03805'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href='https://github.com/yunqing-me/A-Closer-Look-at-FSIG'>[GitHub]</a></span><br>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:22px"><a href='https://drive.google.com/drive/folders/1GkiYFeUd85nDNsrLG52J-xz-jLjiKED3?usp=sharing'>[Data Repository]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p style="text-align:justify; text-justify:inter-ideograph;">
					Modern GANs excel at generating high quality and diverse images. However, when transferring the pretrained GANs on small target data (e.g., 10-shot), the generator tends to replicate the training samples. Several methods have been proposed to address this few-shot image generation task, but there is a lack of effort to analyze them under a unified framework. As our first contribution, we propose a framework to analyze existing methods during the adaptation. Our analysis discovers that while some methods have disproportionate focus on diversity preserving which impede quality improvement, all methods achieve similar quality after convergence. Therefore, the better methods are those that can slow down diversity degradation. Furthermore, our analysis reveals that there is still plenty of room to further slow down diversity degradation. Informed by our analysis and to slow down the diversity degradation of the target generator during adaptation, our second contribution proposes to apply mutual information (MI) maximization to retain the source domainâ€™s rich multi-level diversity information in the target domain generator. We propose to perform MI maximization by contrastive loss (CL), leverage the generator and discriminator as two feature encoders to extract different multi-level features for computing CL. We refer to our method as Dual Contrastive Learning (DCL). Extensive experiments on several public datasets show that, while leading to a slower diversity-degrading generator during adaptation, our proposed DCL brings visually pleasant quality and state-of-the-art quantitative performance.
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<tr>
			<td width=260px>
				<center>
					<img class="round" style="width:850px" src="./resources/degradation.jpg"/>
				</center>
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Overview and our contributions</h1></center>
	<!-- <table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/2b.jpg"/></td>
				</center>
			</td> 
		</tr>
	</table>
	<table align=center width=800px>
		<tr>
			<td>
				<p style="text-align:justify; text-justify:inter-ideograph;">
				 Born-Again Nets in episodic meta-learning (above) and results of our investigation(below): <br>
			</td>
		</tr>
	</table>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:600px" src="./resources/born-again-fsc-results.png"/></td>
				</center>
			</td>
		</tr>
	</table> -->
	<center>
		<table align=center width=880px>
			<tr>
				<td>
					<p style="text-align:justify; text-justify:inter-ideograph;">
					<b>- 1: </b>
					We tackle few-shot image generation (FSIG) via adapting a pre-trained source GAN to a small target domain. <br>
					
					<b>- 2: </b>
					Our work makes two main contributions: 
					<ul>
						<li>
							We discover that, existing methods for FSIG achieve high quality of generated images during adaptation. On the other hand, while different methods can achieve similar quality on the target domain, their diversity-degrading rates
							vary drastically. These observations are shown in the below figure.
						</li>
						<li>
							We propose a new method to improve the performance of FSIG and achieve comparable performance.
						</li>
					</ul>
					<table align=center width=850px>
						<tr>
							<td width=260px>
								<center>
									<img class="round" style="width:880px" src="./resources/quality_eval.jpg"/>
								</center>
							</td>
						</tr>
					</table>
					<table align=center width=850px>
						<tr>
							<td width=260px>
								<center>
									<img class="round" style="width:880px" src="./resources/diversity_eval.jpg"/>
								</center>
							</td>
						</tr>
					</table>
					<b>- 3: </b>
					Schematic diagram of our method:
					We propose dual-contrastive learning (DCL), a mutual-information based method that aims to maximize the mutual information between the source image and the target image that are from the same latent code.
				</td>
				<table align=center width=850px>
					<tr>
						<td width=260px>
							<center>
								<img class="round" style="width:880px" src="./resources/config.jpg"/>
							</center>
						</td>
					</tr>
				</table>
			</tr>
		</table>
	</center>
	<hr>

	<center><h1>Experiment Results</h1></center>
	<table align=center width=880px>
		<tr>
			<td>
				<p style="text-align:justify; text-justify:inter-ideograph;">
				 Comparison with other methods for 10-shot adaptation on FFHQ-Babies and Sketches. Additional result in Supplement.<br>
			</td>
		</tr>
	</table>
	<table align=center width=820px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/babies_compare.jpg"/></td>
				</center>
			</td> 
		</tr>
	</table>
	<table align=center width=800px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/sketches_compare.jpg"/></td>
				</center>
			</td> 
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<p style="text-align:justify; text-justify:inter-ideograph;"> 
					Left: Transferring a GAN pretrained on FFHQ to 10-shot target samples . We fix the noise input by each column to observe the
					relationship of generated images before and after adaptation. Mid: We observe that, most of existing methods lose diversity quickly before
					the quality improvement converges, and tend to replicate the training data. Our method, in contrast, slows down the loss of diversity and
					preserves more details. For example: In red frames (Upper), the hair style and hat are better preserved. In pink frames (Bottom), the smile
					teeth are well inherited from the source domain. We also outperform others in quantitative evaluation (Right).
				</td>
			</tr>
		</center>
	</table>
	<!-- <table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/yunqing-me/Born-Again-FS'>[GitHub]</a>
			</center>
		</span>
	</table> -->
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper Additional Information</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Yunqing Zhao et al.<br>
				<b>A Closer Look at Few-shot Image Generation</b><br>
				In CVPR, 2022.<br>
				(hosted on <a href="https://arxiv.org/pdf/2205.03805.pdf">arXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=800px>
		<tr>
			<td><span style="font-size:14pt"><center>
				If you find our work useful in your research, please consider citing our paper: 
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
					<br>
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>
